# pytorch-note
> 源代码名称加入nn为pytorch官方实现，否则为从0开始实现自己实现
## basis
1. 线性回归实现-linear_regression
2. softmax回归实现-softmax_regression
3. 多层感知机实现-multilayer_perceptron
4. 欠拟合和过拟合-underfitting_overfitting
5. 权重衰减实现-weight_decay
6. 丢弃法实现-dropout
## network
1. 神经网络基础-network_basis
2. GPU基本操作-gpu_basis
3.  卷积神经网络实现-convolutional_neural_network<br>
    填充和步幅-padding_stride<br>
    多个输入通道和输出通道-multiple_input_output_channels<br>
    池化层-pooling_layer <br>
    LeNet实现-lenet<br>
    AlexNet实现-alexnet<br>
    VGG实现-vgg<br>
    NiN实现-nin<br>
    GoogLeNet实现-googlenet<br>
    批量归一化-batch_normalization<br>
    残差网络-resnet<br>
4. 数据增强-data_augmentation<br>
5. 微调-fine_tuning<br>
6.  目标检测和物体检测-object_detection<br>
    锚框-anchor_box<br>
    区域卷积神经网络-rcnn<br>
    SSD实现-ssd<br>
7.  语义分割-semantic_segmentation<br>
    转置卷积-transposed_convolution<br>
    全连接神经网络全卷积网络-fcn<br>
    样式迁移-style_transfer<br>
8.  序列模型-sequence_model<br>
    文本预处理-text_preprocessing<br>
    语言模型-language_model <br>
    循环神经网络-rnn<br>
    门控循环单元-gru<br>
    长短期记忆网络-lstm<br>
    深度循环神经网络-deep_rnn<br>
    双向循环神经网络-bidirectional_rnn<br>
9.  编码器解码器-encoder_decoder<br>
    序列到序列学习-seq2seq<br>
    注意力机制-attention_mechanism<br>
    使用注意力机制的seq2seq-attention_seq2seq<br>
    自注意力机制-self_attention<br>
10.  Transformer-transformer<br>
11. BERT实现-bert<br>
